<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="最近正在嘗試幾種文本分類的算法，卻一直苦於沒有結構化的中文語料，原本是打算先爬下大把大把的部落格文章，再依 tag 將它們分門別類，可惜試了一陣子後，我見識到了理想和現實間的鴻溝。

儘管後來還是搞定了

所以就找上了基於非監督學習的 word2vec，為了銜接後續的資料處理，這邊採用的是基於 python 的主題模型函式庫 gensim。這篇教學並不會談太多 word2vec 的數學原理，而是考慮如何輕鬆又直覺地訓練中文詞向量，文章裡所有的程式碼都會傳上 github，現在，就讓我們進入正題吧。">
    

    <!--Author-->
    
        <meta name="author" content="Justin Yang">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="以 gensim 訓練中文詞向量"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="雷德麥的藏書閣"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>以 gensim 訓練中文詞向量 - 雷德麥的藏書閣</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css" type="text/css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">原站點出了些狀況，過幾天恢復</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/klugjo/hexo-theme-clean-blog">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('http://www.codeblocq.com/assets/projects/hexo-theme-clean-blog/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>以 gensim 訓練中文詞向量</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        2016-08-28
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/Word-Embedding/">#Word Embedding</a> <a href="/tags/Word2Vec/">#Word2Vec</a> <a href="/tags/gensim/">#gensim</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                        

<a href="/categories/自然語言處理/">自然語言處理</a>

                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>最近正在嘗試幾種文本分類的算法，卻一直苦於沒有結構化的中文語料，原本是打算先爬下大把大把的部落格文章，再依 tag 將它們分門別類，可惜試了一陣子後，我見識到了理想和現實間的鴻溝。</p>
<p><img src="http://i.imgur.com/VnjnW7S.jpg" alt="太麻煩的事我不會做"></p>
<p class="img-meta"><a href="https://github.com/zake7749/PTT-Text-Mining" target="_blank" rel="external">儘管後來還是搞定了</a></p>

<p>所以就找上了基於非監督學習的 word2vec，為了銜接後續的資料處理，這邊採用的是基於 python 的主題模型函式庫 <a href="https://radimrehurek.com/gensim/" target="_blank" rel="external">gensim</a>。這篇教學並不會談太多 word2vec 的數學原理，而是考慮如何輕鬆又直覺地訓練中文詞向量，文章裡所有的程式碼都會傳上 <a href="https://github.com/zake7749/word2vec_tutorial" target="_blank" rel="external">github</a>，現在，就讓我們進入正題吧。</p>
<a id="more"></a>
<h2 id="u53D6_u5F97_u8A9E_u6599"><a href="#u53D6_u5F97_u8A9E_u6599" class="headerlink" title="取得語料"></a>取得語料</h2><p>要訓練詞向量，第一步當然是取得資料集。由於 word2vec 是基於非監督式學習，訓練集一定一定要越大越好，語料涵蓋的越全面，訓練出來的結果也會越漂亮。我所採用的是維基百科於<a href="https://dumps.wikimedia.org/zhwiki/20160820/zhwiki-20160820-pages-articles.xml.bz2" target="_blank" rel="external">2016/08/20的備份</a>，文章篇數共有 2822639 篇，如果需要更近期的數據，可以前往<a href="https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD" target="_blank" rel="external">維基百科:資料庫下載</a>，不過請特別注意一點，要挑選的是以 <code>pages-articles.xml.bz2</code> 結尾的備份，而不是以 <code>pages-articles-multistream.xml.bz2</code> 結尾的備份唷，不然會在清理上出現一些異常，無法正常解析文章。</p>
<p>在等待下載的這段時間，我們可以先把這次的主角<code>gensim</code>配置好:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 <span class="keyword">install</span> <span class="comment">--upgrade gensim</span></span><br></pre></td></tr></table></figure>
<p>維基百科下載好後，先別急著解壓縮，因為這是一份 xml 文件，裏頭佈滿了各式各樣的標籤，我們得先想辦法送走這群不速之客，不過也別太擔心，gensim 早已看穿了一切，藉由調用 <a href="https://radimrehurek.com/gensim/corpora/wikicorpus.html" target="_blank" rel="external">wikiCorpus</a>，我們能很輕鬆的只取出文章的標題和內容。</p>
<p>初始化<code>WikiCorpus</code>後，能藉由<code>get_texts()</code>可迭代每一篇文章，它所回傳的是一個tokens list，我以空白符將這些 tokens 串接起來，統一輸出到同一份文字檔裡。這邊要注意一件事，<code>get_texts()</code>受<code>wikicorpus.py</code>中的變數<code>ARTICLE_MIN_WORDS</code>限制，只會回傳內容長度大於 <em>50</em> 的文章。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> WikiCorpus</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">"Usage: python3 "</span> + sys.argv[<span class="number">0</span>] + <span class="string">" wiki_data_path"</span>)</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">    wiki_corpus = WikiCorpus(sys.argv[<span class="number">1</span>], dictionary=&#123;&#125;)</span><br><span class="line">    </span><br><span class="line">    texts_num = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"wiki_texts.txt"</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output:</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> wiki_corpus.get_texts():</span><br><span class="line">            output.write(<span class="string">b' '</span>.join(text).decode(<span class="string">'utf-8'</span>) + <span class="string">'\n'</span>)</span><br><span class="line">            texts_num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> texts_num % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                logging.info(<span class="string">"已處理 %d 篇文章"</span> % texts_num)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>在shell 裡輸入：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 wiki_to_txt<span class="selector-class">.py</span> zhwiki-<span class="number">20160820</span>-pages-articles<span class="selector-class">.xml</span><span class="selector-class">.bz2</span></span><br></pre></td></tr></table></figure>
<p>這約需花費20分鐘(on Mac Air)，就先讓我們看一下接下來還要做些什麼吧~</p>
<h2 id="u958B_u59CB_u65B7_u8A5E"><a href="#u958B_u59CB_u65B7_u8A5E" class="headerlink" title="開始斷詞"></a>開始斷詞</h2><p>我們有清完標籤的語料了，第二件事就是要把語料中每個句子，進一步拆解成一個一個詞，這個步驟稱為「斷詞」。中文斷詞的工具比比皆是，這裏我採用的是 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">jieba</a>，儘管它在繁體中文的斷詞上還是有些不如<code>CKIP</code>，但他實在太簡單、太方便、太好調用了，足以彌補這一點小缺憾：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">快速安裝結巴</span><br><span class="line">pip3 <span class="keyword">install</span> jieba</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 斷詞示例</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="keyword">False</span>)</span><br><span class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#輸出 Default Mode: 我/ 来到/ 北京/ 清华大学</span></span><br></pre></td></tr></table></figure>
<p>現在，我們上一階段的檔案也差不多出爐了，以vi打開看起來會是這個樣子：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">歐幾里得 西元前三世紀的希臘數學家 現在被認為是幾何之父 此畫為拉斐爾的作品 雅典學院 数学 是利用符号语言研究數量 结构 变化以及空间等概念的一門学科 从某种角度看屬於形式科學的一種 數學透過抽象化和邏輯推理的使用 由計數 計算 數學家們拓展這些概念<span class="attr">...</span><span class="attr">...</span></span><br></pre></td></tr></table></figure>
<p>Opps！出了一點狀況，我們發現簡體跟繁體混在一起了，比如「数学」與「數學」會被 word2vec 當成兩個不同的詞，所以我們在斷詞前，還需加上一道繁簡轉換的手續。然而我們的語料集相當龐大，一般的繁簡轉換會有些力不從心，建議採用<a href="https://github.com/BYVoid/OpenCC" target="_blank" rel="external">OpenCC</a>，轉換的方式很簡單：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opencc -<span class="selector-tag">i</span> wiki_texts<span class="selector-class">.txt</span> -o wiki_zh_tw<span class="selector-class">.txt</span> -c s2tw.json</span><br></pre></td></tr></table></figure>
<p>如果是要將繁體轉為簡體，只要將config的參數從<code>s2tw.json</code>改成<code>t2s.json</code>即可。現在再檢查一次<code>wiki_zh_tw.txt</code>，的確只剩下繁體字了，終於能進入斷詞，輸入<code>python3 segment.py</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># jieba custom setting.</span></span><br><span class="line">    jieba.set_dictionary(<span class="string">'jieba_dict/dict.txt.big'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load stopwords set</span></span><br><span class="line">    stopwordset = set()</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'jieba_dict/stopwords.txt'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> sw:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> sw:</span><br><span class="line">            stopwordset.add(line.strip(<span class="string">'\n'</span>))</span><br><span class="line"></span><br><span class="line">    output = open(<span class="string">'wiki_seg.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">    </span><br><span class="line">    texts_num = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'wiki_zh_tw.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> content :</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">            words = jieba.cut(line, cut_all=<span class="keyword">False</span>)</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">                <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwordset:</span><br><span class="line">                    output.write(word +<span class="string">' '</span>)</span><br><span class="line">            texts_num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> texts_num % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                logging.info(<span class="string">"已完成前 %d 行的斷詞"</span> % texts_num)</span><br><span class="line">    output.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	main()</span><br></pre></td></tr></table></figure>
<h3 id="Stopwords_and_Window"><a href="#Stopwords_and_Window" class="headerlink" title="Stopwords and Window"></a>Stopwords and Window</h3><p>好啦，這個東西大概要跑個 80 分鐘，<s>先讓我講些幹話</s>，先讓我們看看上頭做了什麼。除了之前演示的斷詞外，這邊還多做了兩件事，一是調整jieba的辭典，讓他對繁體斷詞比較友善，二是引入了<a href="https://zh.wikipedia.org/wiki/%E5%81%9C%E7%94%A8%E8%AF%8D" target="_blank" rel="external">停用詞</a>，停用詞就是像英文中的 the,a,this，中文的你我他，與其他詞相比顯得不怎麼重要，對文章主題也無關緊要的，就可以將它視為停用詞。而要排除停用詞的理由，其實與word2vec的實作概念大大相關，由於在開頭講明了不深究概念，就讓我舉個例子替代長篇大論。</p>
<p>首先，在word2vec有一個概念叫 windows，我習慣叫他窗口，因為它給我的感覺跟TCP 那個會滑來滑去的東東很像。</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>很顯然，一個詞的意涵跟他的左右鄰居很有關係，比如「雨越下越大，茶越充越淡」，什麼會「下」？「雨」會下，什麼會「淡」？茶會「淡」，這樣的類比舉不勝舉，那麼，若把思維逆轉過來呢？</p>
<p><img src="http://i.imgur.com/KR2ISOg.jpg" alt="逆轉"></p>
<p>顯然，我們或多或少能從左右鄰居是誰，猜出中間的是什麼，這很像我們國高中時天天在練的英文克漏字。那麼問題來了，左右鄰居有誰？能更精確地說，你要往左往右看幾個？假設我們以「孔乙己 一到 店 所有 喝酒 的 人 便都 看著 他 笑」為例，如果往左往右各看一個：</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[孔乙己 一到] 店 所有 喝酒 的 人 便 都 看著 他 笑</span><br><span class="line">[孔乙己 一到 店] 所有 喝酒 的 人 便 都 看著 他 笑</span><br><span class="line">孔乙己 [一到 店 所有] 喝酒 的 人 便 都 看著 他 笑</span><br><span class="line">孔乙己 一到 [店 所有 喝酒] 的 人 便 都 看著 他 笑</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>這樣就構成了一個 size=1 的 windows，這個 1 是極端的例子，為了讓我們看看有停用詞跟沒停用詞差在哪，這句話去除了停用詞應該會變成：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">孔乙己 一到 店 所有 喝酒 人 看著 笑</span><br></pre></td></tr></table></figure>
<p>我們看看「人」的窗口變化，原本是「的 人 便」，後來是「喝酒 人 看著」，相比原本的情形，去除停用詞後，我們對「人」這個詞有更多認識，比如人會喝酒，人會看東西，當然啦，這是我以口語的表達，機器並不會這麼想，機器知道的是人跟喝酒會有某種關聯，跟看會有某種關聯，但儘管如此，也遠比本來的「的 人 便」好太多太多了。</p>
<p>就在剛剛，我的斷詞已經跑完了，現在，讓我們進入收尾的階段吧</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016<span class="selector-tag">-08-26</span> 22<span class="selector-pseudo">:27</span><span class="selector-pseudo">:59</span>,480 : <span class="selector-tag">INFO</span> : 已處理 260000 個 <span class="selector-tag">token</span></span><br></pre></td></tr></table></figure>
<h2 id="u8A13_u7DF4_u8A5E_u5411_u91CF"><a href="#u8A13_u7DF4_u8A5E_u5411_u91CF" class="headerlink" title="訓練詞向量"></a>訓練詞向量</h2><p>這是最簡單的部分，同時也是最困難的部分，簡單的是程式碼，困難的是詞向量效能上的微調與後訓練。對了，如果你已經對詞向量和語言模型有些研究，在輸入<code>python3 train.py</code>之前，建議先看一下之後的內文，相信我，你會需要的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">    </span><br><span class="line">    sentences = word2vec.Text8Corpus(<span class="string">"wiki_seg.txt"</span>)</span><br><span class="line">    model = word2vec.Word2Vec(sentences, size=<span class="number">250</span>)</span><br><span class="line">    model.save_word2vec_format(<span class="string">u"med250.model.bin"</span>, binary=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># how to load a model ?</span></span><br><span class="line">    <span class="comment"># model = word2vec.Word2Vec.load_word2vec_format("your_model.bin", binary=True)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>扣掉<code>logging</code>與註釋就剩下三行，真是精簡的漂亮。上頭通篇的學問在<code>model = word2vec.Word2Vec(sentences, size=250)</code>，我們先讓它現出原型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">gensim</span>.<span class="title">models</span>.<span class="title">word2vec</span>.<span class="title">Word2Vec</span><span class="params">(sentences=None, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=None, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-in function hash&gt;, iter=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=None, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>這抵得上 train.py 的所有程式碼了。不過也別太擔心，裏頭多數是無關緊要的參數，比較關鍵的大概是：</p>
<ul>
<li>sentences:當然了，這是要訓練的句子集，沒有他就不用跑了</li>
<li>size:這表示的是訓練出的詞向量會有幾維</li>
<li>alpha:機器學習中的學習率，這東西會逐漸收斂到 <code>min_alpha</code></li>
<li>sg:這個不是三言兩語能說完的，sg=1表示採用skip-gram,sg=0 表示採用cbow</li>
<li>window:還記得孔乙己的例子嗎？往左往右看幾個字的意思，印象中原作者論文裡寫 cbow 採用 5 是不錯的選擇</li>
<li>workers:執行緒數目，除非電腦不錯，不然建議別超過 4</li>
<li>min_count:若這個詞出現的次數小於<code>min_count</code>，那他就不會被視為訓練對象</li>
</ul>
<h2 id="u8A5E_u5411_u91CF_u5BE6_u9A57"><a href="#u8A5E_u5411_u91CF_u5BE6_u9A57" class="headerlink" title="詞向量實驗"></a>詞向量實驗</h2><p>訓練完成後，讓我們來測試一下模型的效能，運行<code>python3 demo.py</code>。由於 gensim 將整個模型讀了進來並作了一層映射，所以記憶體會消耗相當多，如果出現了<code>MemoryError</code>，可能得調整一下<code>min_count</code>或對常用詞作一層快取，這點要特別注意一下。</p>
<p>先來試試相似詞排序吧！</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">飲料</span><br><span class="line"></span><br><span class="line">相似詞前 100 排序</span><br><span class="line">飲品,0<span class="selector-class">.8439314365386963</span></span><br><span class="line">果汁,0<span class="selector-class">.7858869433403015</span></span><br><span class="line">罐裝,0<span class="selector-class">.7305712699890137</span></span><br><span class="line">冰淇淋,0<span class="selector-class">.702262818813324</span></span><br><span class="line">酸奶,0<span class="selector-class">.7007108926773071</span></span><br><span class="line">口香糖,0<span class="selector-class">.6987590193748474</span></span><br><span class="line">酒類,0<span class="selector-class">.6967358589172363</span></span><br><span class="line">可口可樂,0<span class="selector-class">.6885123252868652</span></span><br><span class="line">酒精類,0<span class="selector-class">.6843742728233337</span></span><br><span class="line">含酒精,0<span class="selector-class">.6825539469718933</span></span><br><span class="line">啤酒,0<span class="selector-class">.6816493272781372</span></span><br><span class="line">薯片,0<span class="selector-class">.6779764294624329</span></span><br><span class="line">紅茶,0<span class="selector-class">.6656282544136047</span></span><br><span class="line">奶茶,0<span class="selector-class">.656740128993988</span></span><br><span class="line">提神,0<span class="selector-class">.6566425561904907</span></span><br><span class="line">牛奶,0<span class="selector-class">.6556192636489868</span></span><br><span class="line">檸檬茶,0<span class="selector-class">.6494661569595337</span></span><br><span class="line"></span><br><span class="line">籃球</span><br><span class="line"></span><br><span class="line">相似詞前 100 排序</span><br><span class="line">美式足球,0<span class="selector-class">.6463411450386047</span></span><br><span class="line">橄欖球,0<span class="selector-class">.6382837891578674</span></span><br><span class="line">男子籃球,0<span class="selector-class">.6187020540237427</span></span><br><span class="line">冰球,0<span class="selector-class">.6056296825408936</span></span><br><span class="line">棒球,0<span class="selector-class">.5859025716781616</span></span><br><span class="line">籃球運動,0<span class="selector-class">.5831792950630188</span></span><br><span class="line">籃球員,0<span class="selector-class">.5782726407051086</span></span><br><span class="line">籃球隊,0<span class="selector-class">.576259195804596</span></span><br><span class="line">排球,0<span class="selector-class">.5743488073348999</span></span><br><span class="line">黑子,0<span class="selector-class">.5609416961669922</span></span><br><span class="line">籃球比賽,0<span class="selector-class">.5498511791229248</span></span><br><span class="line">打球,0<span class="selector-class">.5496408939361572</span></span><br><span class="line">中國籃球,0<span class="selector-class">.5471529960632324</span></span><br><span class="line">男籃,0<span class="selector-class">.5460700392723083</span></span><br><span class="line"><span class="selector-tag">ncaa</span>,0<span class="selector-class">.543986439704895</span></span><br><span class="line">投投,0<span class="selector-class">.5439497232437134</span></span><br><span class="line">曲棍球,0<span class="selector-class">.5435376167297363</span></span><br><span class="line"><span class="selector-tag">nba</span>,0<span class="selector-class">.5415610671043396</span></span><br></pre></td></tr></table></figure>
<p>前100太多了，所以只把前幾個結果貼上來，我們也能調用<code>model.similarity(word2,word1)</code>來直接取得兩個詞的相似度：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">冰沙 刨冰</span><br><span class="line">計算 <span class="selector-tag">Cosine</span> 相似度</span><br><span class="line">0<span class="selector-class">.631961417455</span></span><br><span class="line"></span><br><span class="line">電腦 飛鏢</span><br><span class="line">計算 <span class="selector-tag">Cosine</span> 相似度</span><br><span class="line">0<span class="selector-class">.154503715708</span></span><br><span class="line"></span><br><span class="line">電腦 程式</span><br><span class="line">計算 <span class="selector-tag">Cosine</span> 相似度</span><br><span class="line">0<span class="selector-class">.5021829415</span></span><br><span class="line"></span><br><span class="line">衛生紙 漫畫</span><br><span class="line">計算 <span class="selector-tag">Cosine</span> 相似度</span><br><span class="line">0<span class="selector-class">.167776641495</span></span><br></pre></td></tr></table></figure>
<p>能稍微區隔出詞與詞之間的主題，整體來說算是可以接受的了。</p>
<h3 id="u66F4_u4E0A_u4E00_u5C64_u6A13"><a href="#u66F4_u4E0A_u4E00_u5C64_u6A13" class="headerlink" title="更上一層樓"></a>更上一層樓</h3><p>如何優化詞向量的表現？這其實有蠻多方法的，大方向是從應用的角度出發，我們能針對應用特化的語料進行再訓練，除此之外，斷詞器的選擇也很重要，它很大程度的決定什麼詞該在什麼地方出現，如果發現 <code>jieba</code> 有些力不能及的，不妨試著採用別的斷詞器，或是試著在 <code>jieba</code> 自訂辭典，調一下每個詞的權重。</p>
<p>應用考慮好了，接著看看模型，我們可以調整 <code>model()</code> 的參數，比方窗口大小、維度、學習率，進一步還能比較 skip-gram 與 cbow 的效能差異，什麼，你說不知道 skip-gram 跟 cbow 是什麼？且看下回分解。</p>
<h2 id="u53C3_u8003_u8CC7_u6599"><a href="#u53C3_u8003_u8CC7_u6599" class="headerlink" title="參考資料"></a>參考資料</h2><p><a href="http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim" target="_blank" rel="external">Training Word2Vec Model on English Wikipedia by Gensim</a></p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    

                    
                </ul>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



</body>

</html>